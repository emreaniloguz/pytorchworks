{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EMNIST_CNN.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4140df27fa78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#Read Weights:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'EMNIST_CNN.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EMNIST_CNN.pkl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torchvision import utils\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.Conv1 = nn.Sequential(\n",
    "            #Convolution Level 1\n",
    "            nn.Conv2d(1, 16, 5, 1, 2),\n",
    "            #Activation function layer\n",
    "            nn.ReLU(),\n",
    "            #Maximum pooled layer\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        self.Conv2 = nn.Sequential(\n",
    "            #Convolution Layer 2\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            #Activation function layer\n",
    "            nn.ReLU(),\n",
    "            #Maximum pooled layer\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        #Finally, attach a fully connected layer (to make the image one-dimensional)\n",
    "        #Why 32*7*7:(1,28,28) -> (16,28,28) (conv1) -> (16,14,14) (pool1) -> (32,14,14) (conv2) -> (32,7,7) (pool2) ->output\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(32*7*7,400),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,26),\n",
    "         )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.Conv1(input)\n",
    "        input = self.Conv2(input)       #view can be understood as resize\n",
    "        #input.size() = [100, 32, 7, 7], 100 is quantity per batch, 32 is thickness, picture size is 7*7\n",
    "        #When a dimension is -1, its size is automatically calculated (the principle is that the total amount of data is constant):\n",
    "        input = input.view(input.size(0), -1) #(batch=100, 1568), the end result is to compress a two-dimensional picture into one dimension (the amount of data remains constant)\n",
    "        #Finally, connect to a full connection layer with output of 10:[100,1568]*[1568,10]=[100,10]\n",
    "        output = self.Linear(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read Network Framework\n",
    "cnn = CNN()\n",
    "#Read Weights:\n",
    "cnn.load_state_dict(torch.load('EMNIST_CNN.pkl'))\n",
    "\n",
    "\n",
    "#test_x:(10000 rows and 1 column, each column element is 28*28 matrix) \n",
    "# Provide your own data for testing:\n",
    "my_img = plt.imread(\"Emnist_letters_png/My_jpg/g.jpg\")\n",
    "my_img = my_img[:,:,0] #Convert to Single Channel\n",
    "my_img = cv2.resize(my_img,(28,28))#Convert to 28*28 size\n",
    "my_img = torch.from_numpy(my_img)#Convert to Tensor\n",
    "my_img = torch.unsqueeze(my_img, dim = 0)#Add a dimension\n",
    "my_img = torch.unsqueeze(my_img, dim = 0)/255. #Add another dimension and map the gray scale between (0,1)       \n",
    "#print(my_img.size())#torch.Size([1, 1, 28, 28]) Convolution layer requires four dimensions of input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Visualization section:\n",
    "\n",
    "#Input original image:\n",
    "plt.imshow(my_img.squeeze())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Conv1:\n",
    "cnt = 1\n",
    "my_img = cnn.Conv1(my_img)\n",
    "img = my_img.squeeze()\n",
    "for i in img.squeeze():\n",
    "\n",
    "    plt.axis('off')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5,5)#Output width*height pixels\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    plt.imshow(i.detach().numpy())\n",
    "    plt.subplot(4, 4, cnt)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(i.detach().numpy())\n",
    "    cnt += 1\n",
    "plt.subplots_adjust(top=1,bottom=0,left=0,right=1,hspace=0,wspace=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Conv2:\n",
    "cnt = 1\n",
    "my_img = cnn.Conv2(my_img)\n",
    "img = my_img.squeeze()\n",
    "for i in img.squeeze():\n",
    "\n",
    "    plt.axis('off')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5,5)#Output width*height pixels\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    plt.imshow(i.detach().numpy())\n",
    "    plt.subplot(4, 8, cnt)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(i.detach().numpy())\n",
    "    cnt += 1\n",
    "#plt.subplots_adjust(top=1,bottom=0,left=0,right=1,hspace=0,wspace=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Full Connection Layer:\n",
    "my_img = my_img.view(my_img.size(0), -1)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10000,4)#Output width*height pixels\n",
    "plt.subplots_adjust(top=1,bottom=0,left=0,right=1,hspace=0,wspace=0)\n",
    "plt.margins(0,0)\n",
    "\n",
    "\n",
    "my_img = cnn.Linear[0](my_img)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(my_img.detach().numpy())\n",
    "\n",
    "my_img = cnn.Linear[1](my_img)\n",
    "my_img = cnn.Linear[2](my_img)\n",
    "my_img = cnn.Linear[3](my_img)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(my_img.detach().numpy())\n",
    "\n",
    "my_img = cnn.Linear[4](my_img)\n",
    "my_img = cnn.Linear[5](my_img)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(my_img.detach().numpy())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Output prediction results:\n",
    "pred_y = int(torch.max(my_img,1)[1])\n",
    "#chr() converts a number to the corresponding ASCII character\n",
    "print('\\npredict character: %c or %c' % (chr(pred_y+65),chr(pred_y+97)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75a30548fdf3dbc5d56c450174bec395510dfa79d754af80d480ee316ccf8c82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
